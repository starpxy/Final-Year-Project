{"author": "tensorflow", "code": "\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================\n\n This is the complete code for the following blogpost:\n https://developers.googleblog.com/2017/12/creating-custom-estimators-in-tensorflow.html\n\nimport tensorflow as tf\nimport os\nimport sys\n\nimport six.moves.urllib.request as request\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n Check that we have correct TensorFlow version installed\ntf_version = tf.__version__\ntf.logging.info(\"TensorFlow version: {}\".format(tf_version))\nassert \"1.4\" <= tf_version, \"TensorFlow r1.4 or later is needed\"\n\n Windows users: You only need to change PATH, rest is platform independent\nPATH = \"/tmp/tf_custom_estimators\"\n\n Fetch and store Training and Test dataset files\nPATH_DATASET = PATH + os.sep + \"dataset\"\nFILE_TRAIN = PATH_DATASET + os.sep + \"iris_training.csv\"\nFILE_TEST = PATH_DATASET + os.sep + \"iris_test.csv\"\nURL_TRAIN = \"http://download.tensorflow.org/data/iris_training.csv\"\nURL_TEST = \"http://download.tensorflow.org/data/iris_test.csv\"\n\ndef downloadDataset(url, file):\n    if not os.path.exists(PATH_DATASET):\n        os.makedirs(PATH_DATASET)\n    if not os.path.exists(file):\n        data = request.urlopen(url).read()\n        with open(file, \"wb\") as f:\n            f.write(data)\n            f.close()\ndownloadDataset(URL_TRAIN, FILE_TRAIN)\ndownloadDataset(URL_TEST, FILE_TEST)\n\n The CSV features in our training & test data\nfeature_names = [\n    'SepalLength',\n    'SepalWidth',\n    'PetalLength',\n    'PetalWidth']\n\n Create an input function reading a file using the Dataset API\n Then provide the results to the Estimator API\ndef my_input_fn(file_path, repeat_count=1, shuffle_count=1):\n    def decode_csv(line):\n        parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]])\n        label = parsed_line[-1]   Last element is the label\n        del parsed_line[-1]   Delete last element\n        features = parsed_line   Everything but last elements are the features\n        d = dict(zip(feature_names, features)), label\n        return d\n\n    dataset = (tf.data.TextLineDataset(file_path)   Read text file\n        .skip(1)   Skip header row\n        .map(decode_csv, num_parallel_calls=4)   Decode each line\n        .cache()  Warning: Caches entire dataset, can cause out of memory\n        .shuffle(shuffle_count)   Randomize elems (1 == no operation)\n        .repeat(repeat_count)     Repeats dataset this  times\n        .batch(32)\n        .prefetch(1)   Make sure you always have 1 batch ready to serve\n    )\n    iterator = dataset.make_one_shot_iterator()\n    batch_features, batch_labels = iterator.get_next()\n    return batch_features, batch_labels\n\ndef my_model_fn(\n    features,  This is batch_features from input_fn\n    labels,    This is batch_labels from input_fn\n    mode):     And instance of tf.estimator.ModeKeys, see below\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        tf.logging.info(\"my_model_fn: PREDICT, {}\".format(mode))\n    elif mode == tf.estimator.ModeKeys.EVAL:\n        tf.logging.info(\"my_model_fn: EVAL, {}\".format(mode))\n    elif mode == tf.estimator.ModeKeys.TRAIN:\n        tf.logging.info(\"my_model_fn: TRAIN, {}\".format(mode))\n\n     All our inputs are feature columns of type numeric_column\n    feature_columns = [\n        tf.feature_column.numeric_column(feature_names[0]),\n        tf.feature_column.numeric_column(feature_names[1]),\n        tf.feature_column.numeric_column(feature_names[2]),\n        tf.feature_column.numeric_column(feature_names[3])\n    ]\n\n     Create the layer of input\n    input_layer = tf.feature_column.input_layer(features, feature_columns)\n\n     Definition of hidden layer: h1\n     We implement it as a fully-connected layer (tf.layers.dense)\n     Has 10 neurons, and uses ReLU as the activation function\n     Takes input_layer as input\n    h1 = tf.layers.Dense(10, activation=tf.nn.relu)(input_layer)\n\n     Definition of hidden layer: h2 (this is the logits layer)\n     Similar to h1, but takes h1 as input\n    h2 = tf.layers.Dense(10, activation=tf.nn.relu)(h1)\n\n     Output 'logits' layer is three number = probability distribution\n     between Iris Setosa, Versicolor, and Viginica\n    logits = tf.layers.Dense(3)(h2)\n\n     class_ids will be the model prediction for the class (Iris flower type)\n     The output node with the highest value is our prediction\n    predictions = { 'class_ids': tf.argmax(input=logits, axis=1) }\n\n     1. Prediction mode\n     Return our prediction\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n     Evaluation and Training mode\n\n     Calculate the loss\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n\n     Calculate the accuracy between the true labels, and our predictions\n    accuracy = tf.metrics.accuracy(labels, predictions['class_ids'])\n\n     2. Evaluation mode\n     Return our loss (which is used to evaluate our model)\n     Set the TensorBoard scalar my_accurace to the accuracy\n     Obs: This function only sets value during mode == ModeKeys.EVAL\n     To set values during training, see tf.summary.scalar\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(\n            mode,\n            loss=loss,\n            eval_metric_ops={'my_accuracy': accuracy})\n\n     If mode is not PREDICT nor EVAL, then we must be in TRAIN\n    assert mode == tf.estimator.ModeKeys.TRAIN, \"TRAIN is only ModeKey left\"\n\n     3. Training mode\n\n     Default optimizer for DNNClassifier: Adagrad with learning rate=0.05\n     Our objective (train_op) is to minimize loss\n     Provide global step counter (used to count gradient updates)\n    optimizer = tf.train.AdagradOptimizer(0.05)\n    train_op = optimizer.minimize(\n        loss,\n        global_step=tf.train.get_global_step())\n\n     Set the TensorBoard scalar my_accuracy to the accuracy\n     Obs: This function only sets the value during mode == ModeKeys.TRAIN\n     To set values during evaluation, see eval_metrics_ops\n    tf.summary.scalar('my_accuracy', accuracy[1])\n\n     Return training operations: loss and train_op\n    return tf.estimator.EstimatorSpec(\n        mode,\n        loss=loss,\n        train_op=train_op)\n\n Create a custom estimator using my_model_fn to define the model\ntf.logging.info(\"Before classifier construction\")\nclassifier = tf.estimator.Estimator(\n    model_fn=my_model_fn,\n    model_dir=PATH)   Path to where checkpoints etc are stored\ntf.logging.info(\"...done constructing classifier\")\n\n 500 epochs = 500 * 120 records [60000] = (500 * 120) / 32 batches = 1875 batches\n 4 epochs = 4 * 30 records = (4 * 30) / 32 batches = 3.75 batches\n\n Train our model, use the previously function my_input_fn\n Input to training is a file with training example\n Stop training after 8 iterations of train data (epochs)\ntf.logging.info(\"Before classifier.train\")\nclassifier.train(\n    input_fn=lambda: my_input_fn(FILE_TRAIN, 500, 256))\ntf.logging.info(\"...done classifier.train\")\n\n Evaluate our model using the examples contained in FILE_TEST\n Return value will contain evaluation_metrics such as: loss & average_loss\ntf.logging.info(\"Before classifier.evaluate\")\nevaluate_result = classifier.evaluate(\n    input_fn=lambda: my_input_fn(FILE_TEST, 4))\ntf.logging.info(\"...done classifier.evaluate\")\ntf.logging.info(\"Evaluation results\")\nfor key in evaluate_result:\n    tf.logging.info(\"   {}, was: {}\".format(key, evaluate_result[key]))\n\n Predict the type of some Iris flowers.\n Let's predict the examples in FILE_TEST, repeat only once.\npredict_results = classifier.predict(\n    input_fn=lambda: my_input_fn(FILE_TEST, 1))\ntf.logging.info(\"Prediction on test file\")\nfor prediction in predict_results:\n     Will print the predicted class, i.e: 0, 1, or 2 if the prediction\n     is Iris Setosa, Vericolor, Virginica, respectively.\n    tf.logging.info(\"...{}\".format(prediction[\"class_ids\"]))\n\n Let create a dataset for prediction\n We've taken the first 3 examples in FILE_TEST\nprediction_input = [[5.9, 3.0, 4.2, 1.5],   -> 1, Iris Versicolor\n                    [6.9, 3.1, 5.4, 2.1],   -> 2, Iris Virginica\n                    [5.1, 3.3, 1.7, 0.5]]   -> 0, Iris Setosa\n\ndef new_input_fn():\n    def decode(x):\n        x = tf.split(x, 4)   Need to split into our 4 features\n        return dict(zip(feature_names, x))   To build a dict of them\n\n    dataset = tf.data.Dataset.from_tensor_slices(prediction_input)\n    dataset = dataset.map(decode)\n    iterator = dataset.make_one_shot_iterator()\n    next_feature_batch = iterator.get_next()\n    return next_feature_batch, None   In prediction, we have no labels\n\n Predict all our prediction_input\npredict_results = classifier.predict(input_fn=new_input_fn)\n\n Print results\ntf.logging.info(\"Predictions on memory\")\nfor idx, prediction in enumerate(predict_results):\n    type = prediction[\"class_ids\"]   Get the predicted class (index)\n    if type == 0:\n        tf.logging.info(\"...I think: {}, is Iris Setosa\".format(prediction_input[idx]))\n    elif type == 1:\n        tf.logging.info(\"...I think: {}, is Iris Versicolor\".format(prediction_input[idx]))\n    else:\n        tf.logging.info(\"...I think: {}, is Iris Virginica\".format(prediction_input[idx]))\n", "comments": "  copyright 2017 the tensorflow authors  all rights reserved        licensed apache license  version 2 0 (the  license )     may use file except compliance license     you may obtain copy license           http   www apache org licenses license 2 0       unless required applicable law agreed writing  software    distributed license distributed  as is  basis     without warranties or conditions of any kind  either express implied     see license specific language governing permissions    limitations license                                                                                       this complete code following blogpost     https   developers googleblog com 2017 12 creating custom estimators tensorflow html    check correct tensorflow version installed    windows users  you need change path  rest platform independent    fetch store training test dataset files    the csv features training   test data    create input function reading file using dataset api    then provide results estimator api    last element label    delete last element    everything last elements features    read text file    skip header row    decode line    warning  caches entire dataset  cause memory    randomize elems (1    operation)    repeats dataset   times    make sure always 1 batch ready serve    this batch features input fn    this batch labels input fn    and instance tf estimator modekeys  see    all inputs feature columns type numeric column    create layer input    definition hidden layer  h1    we implement fully connected layer (tf layers dense)    has 10 neurons  uses relu activation function    takes input layer input    definition hidden layer  h2 (this logits layer)    similar h1  takes h1 input    output  logits  layer three number   probability distribution    iris setosa  versicolor  viginica    class ids model prediction class (iris flower type)    the output node highest value prediction    1  prediction mode    return prediction    evaluation training mode    calculate loss    calculate accuracy true labels  predictions    2  evaluation mode    return loss (which used evaluate model)    set tensorboard scalar accurace accuracy    obs  this function sets value mode    modekeys eval    to set values training  see tf summary scalar    if mode predict eval  must train    3  training mode    default optimizer dnnclassifier  adagrad learning rate 0 05    our objective (train op) minimize loss    provide global step counter (used count gradient updates)    set tensorboard scalar accuracy accuracy    obs  this function sets value mode    modekeys train    to set values evaluation  see eval metrics ops    return training operations  loss train op    create custom estimator using model fn define model    path checkpoints etc stored    500 epochs   500   120 records  60000    (500   120)   32 batches   1875 batches    4 epochs   4   30 records   (4   30)   32 batches   3 75 batches    train model  use previously function input fn    input training file training example    stop training 8 iterations train data (epochs)    evaluate model using examples contained file test    return value contain evaluation metrics  loss   average loss    predict type iris flowers     let predict examples file test  repeat     will print predicted class  e  0  1  2 prediction    iris setosa  vericolor  virginica  respectively     let create dataset prediction    we taken first 3 examples file test       1  iris versicolor       2  iris virginica       0  iris setosa    need split 4 features    to build dict    in prediction  labels    predict prediction input    print results    get predicted class (index) ", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# This is the complete code for the following blogpost:\n# https://developers.googleblog.com/2017/12/creating-custom-estimators-in-tensorflow.html\n\nimport tensorflow as tf\nimport os\nimport sys\n\nimport six.moves.urllib.request as request\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n# Check that we have correct TensorFlow version installed\ntf_version = tf.__version__\ntf.logging.info(\"TensorFlow version: {}\".format(tf_version))\nassert \"1.4\" <= tf_version, \"TensorFlow r1.4 or later is needed\"\n\n# Windows users: You only need to change PATH, rest is platform independent\nPATH = \"/tmp/tf_custom_estimators\"\n\n# Fetch and store Training and Test dataset files\nPATH_DATASET = PATH + os.sep + \"dataset\"\nFILE_TRAIN = PATH_DATASET + os.sep + \"iris_training.csv\"\nFILE_TEST = PATH_DATASET + os.sep + \"iris_test.csv\"\nURL_TRAIN = \"http://download.tensorflow.org/data/iris_training.csv\"\nURL_TEST = \"http://download.tensorflow.org/data/iris_test.csv\"\n\ndef downloadDataset(url, file):\n    if not os.path.exists(PATH_DATASET):\n        os.makedirs(PATH_DATASET)\n    if not os.path.exists(file):\n        data = request.urlopen(url).read()\n        with open(file, \"wb\") as f:\n            f.write(data)\n            f.close()\ndownloadDataset(URL_TRAIN, FILE_TRAIN)\ndownloadDataset(URL_TEST, FILE_TEST)\n\n# The CSV features in our training & test data\nfeature_names = [\n    'SepalLength',\n    'SepalWidth',\n    'PetalLength',\n    'PetalWidth']\n\n# Create an input function reading a file using the Dataset API\n# Then provide the results to the Estimator API\ndef my_input_fn(file_path, repeat_count=1, shuffle_count=1):\n    def decode_csv(line):\n        parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]])\n        label = parsed_line[-1]  # Last element is the label\n        del parsed_line[-1]  # Delete last element\n        features = parsed_line  # Everything but last elements are the features\n        d = dict(zip(feature_names, features)), label\n        return d\n\n    dataset = (tf.data.TextLineDataset(file_path)  # Read text file\n        .skip(1)  # Skip header row\n        .map(decode_csv, num_parallel_calls=4)  # Decode each line\n        .cache() # Warning: Caches entire dataset, can cause out of memory\n        .shuffle(shuffle_count)  # Randomize elems (1 == no operation)\n        .repeat(repeat_count)    # Repeats dataset this # times\n        .batch(32)\n        .prefetch(1)  # Make sure you always have 1 batch ready to serve\n    )\n    iterator = dataset.make_one_shot_iterator()\n    batch_features, batch_labels = iterator.get_next()\n    return batch_features, batch_labels\n\ndef my_model_fn(\n    features, # This is batch_features from input_fn\n    labels,   # This is batch_labels from input_fn\n    mode):    # And instance of tf.estimator.ModeKeys, see below\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        tf.logging.info(\"my_model_fn: PREDICT, {}\".format(mode))\n    elif mode == tf.estimator.ModeKeys.EVAL:\n        tf.logging.info(\"my_model_fn: EVAL, {}\".format(mode))\n    elif mode == tf.estimator.ModeKeys.TRAIN:\n        tf.logging.info(\"my_model_fn: TRAIN, {}\".format(mode))\n\n    # All our inputs are feature columns of type numeric_column\n    feature_columns = [\n        tf.feature_column.numeric_column(feature_names[0]),\n        tf.feature_column.numeric_column(feature_names[1]),\n        tf.feature_column.numeric_column(feature_names[2]),\n        tf.feature_column.numeric_column(feature_names[3])\n    ]\n\n    # Create the layer of input\n    input_layer = tf.feature_column.input_layer(features, feature_columns)\n\n    # Definition of hidden layer: h1\n    # We implement it as a fully-connected layer (tf.layers.dense)\n    # Has 10 neurons, and uses ReLU as the activation function\n    # Takes input_layer as input\n    h1 = tf.layers.Dense(10, activation=tf.nn.relu)(input_layer)\n\n    # Definition of hidden layer: h2 (this is the logits layer)\n    # Similar to h1, but takes h1 as input\n    h2 = tf.layers.Dense(10, activation=tf.nn.relu)(h1)\n\n    # Output 'logits' layer is three number = probability distribution\n    # between Iris Setosa, Versicolor, and Viginica\n    logits = tf.layers.Dense(3)(h2)\n\n    # class_ids will be the model prediction for the class (Iris flower type)\n    # The output node with the highest value is our prediction\n    predictions = { 'class_ids': tf.argmax(input=logits, axis=1) }\n\n    # 1. Prediction mode\n    # Return our prediction\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n    # Evaluation and Training mode\n\n    # Calculate the loss\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n\n    # Calculate the accuracy between the true labels, and our predictions\n    accuracy = tf.metrics.accuracy(labels, predictions['class_ids'])\n\n    # 2. Evaluation mode\n    # Return our loss (which is used to evaluate our model)\n    # Set the TensorBoard scalar my_accurace to the accuracy\n    # Obs: This function only sets value during mode == ModeKeys.EVAL\n    # To set values during training, see tf.summary.scalar\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(\n            mode,\n            loss=loss,\n            eval_metric_ops={'my_accuracy': accuracy})\n\n    # If mode is not PREDICT nor EVAL, then we must be in TRAIN\n    assert mode == tf.estimator.ModeKeys.TRAIN, \"TRAIN is only ModeKey left\"\n\n    # 3. Training mode\n\n    # Default optimizer for DNNClassifier: Adagrad with learning rate=0.05\n    # Our objective (train_op) is to minimize loss\n    # Provide global step counter (used to count gradient updates)\n    optimizer = tf.train.AdagradOptimizer(0.05)\n    train_op = optimizer.minimize(\n        loss,\n        global_step=tf.train.get_global_step())\n\n    # Set the TensorBoard scalar my_accuracy to the accuracy\n    # Obs: This function only sets the value during mode == ModeKeys.TRAIN\n    # To set values during evaluation, see eval_metrics_ops\n    tf.summary.scalar('my_accuracy', accuracy[1])\n\n    # Return training operations: loss and train_op\n    return tf.estimator.EstimatorSpec(\n        mode,\n        loss=loss,\n        train_op=train_op)\n\n# Create a custom estimator using my_model_fn to define the model\ntf.logging.info(\"Before classifier construction\")\nclassifier = tf.estimator.Estimator(\n    model_fn=my_model_fn,\n    model_dir=PATH)  # Path to where checkpoints etc are stored\ntf.logging.info(\"...done constructing classifier\")\n\n# 500 epochs = 500 * 120 records [60000] = (500 * 120) / 32 batches = 1875 batches\n# 4 epochs = 4 * 30 records = (4 * 30) / 32 batches = 3.75 batches\n\n# Train our model, use the previously function my_input_fn\n# Input to training is a file with training example\n# Stop training after 8 iterations of train data (epochs)\ntf.logging.info(\"Before classifier.train\")\nclassifier.train(\n    input_fn=lambda: my_input_fn(FILE_TRAIN, 500, 256))\ntf.logging.info(\"...done classifier.train\")\n\n# Evaluate our model using the examples contained in FILE_TEST\n# Return value will contain evaluation_metrics such as: loss & average_loss\ntf.logging.info(\"Before classifier.evaluate\")\nevaluate_result = classifier.evaluate(\n    input_fn=lambda: my_input_fn(FILE_TEST, 4))\ntf.logging.info(\"...done classifier.evaluate\")\ntf.logging.info(\"Evaluation results\")\nfor key in evaluate_result:\n    tf.logging.info(\"   {}, was: {}\".format(key, evaluate_result[key]))\n\n# Predict the type of some Iris flowers.\n# Let's predict the examples in FILE_TEST, repeat only once.\npredict_results = classifier.predict(\n    input_fn=lambda: my_input_fn(FILE_TEST, 1))\ntf.logging.info(\"Prediction on test file\")\nfor prediction in predict_results:\n    # Will print the predicted class, i.e: 0, 1, or 2 if the prediction\n    # is Iris Setosa, Vericolor, Virginica, respectively.\n    tf.logging.info(\"...{}\".format(prediction[\"class_ids\"]))\n\n# Let create a dataset for prediction\n# We've taken the first 3 examples in FILE_TEST\nprediction_input = [[5.9, 3.0, 4.2, 1.5],  # -> 1, Iris Versicolor\n                    [6.9, 3.1, 5.4, 2.1],  # -> 2, Iris Virginica\n                    [5.1, 3.3, 1.7, 0.5]]  # -> 0, Iris Setosa\n\ndef new_input_fn():\n    def decode(x):\n        x = tf.split(x, 4)  # Need to split into our 4 features\n        return dict(zip(feature_names, x))  # To build a dict of them\n\n    dataset = tf.data.Dataset.from_tensor_slices(prediction_input)\n    dataset = dataset.map(decode)\n    iterator = dataset.make_one_shot_iterator()\n    next_feature_batch = iterator.get_next()\n    return next_feature_batch, None  # In prediction, we have no labels\n\n# Predict all our prediction_input\npredict_results = classifier.predict(input_fn=new_input_fn)\n\n# Print results\ntf.logging.info(\"Predictions on memory\")\nfor idx, prediction in enumerate(predict_results):\n    type = prediction[\"class_ids\"]  # Get the predicted class (index)\n    if type == 0:\n        tf.logging.info(\"...I think: {}, is Iris Setosa\".format(prediction_input[idx]))\n    elif type == 1:\n        tf.logging.info(\"...I think: {}, is Iris Versicolor\".format(prediction_input[idx]))\n    else:\n        tf.logging.info(\"...I think: {}, is Iris Virginica\".format(prediction_input[idx]))\n", "description": "Models and examples built with TensorFlow", "file_name": "blog_custom_estimators.py", "id": "3eb75ad51e5eb3e0623ea068c1796dc3", "language": "Python", "project_name": "models", "quality": "", "save_path": "/home/ubuntu/test_files/clean/network_test/tensorflow-models/tensorflow-models-086d914/samples/outreach/blogs/blog_custom_estimators.py", "save_time": "", "source": "", "update_at": "2018-03-14T01:59:19Z", "url": "https://github.com/tensorflow/models", "wiki": true}